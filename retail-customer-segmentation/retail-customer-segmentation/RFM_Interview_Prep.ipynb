{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3874c99",
   "metadata": {},
   "source": [
    "## 1. Executive Summary (30-second pitch)\n",
    "\n",
    "**What I did:**\n",
    "> \"I segmented 4,372 e-commerce customers using RFM analysis, identifying that 25% of customers (Champions) drive 60% of revenue. I validated the segmentation using ANOVA (p < 0.001) and compared it against k-means clustering to ensure the approach wasn't arbitrary.\"\n",
    "\n",
    "**Why it matters:**\n",
    "> \"This enables targeted marketing: VIP programs for Champions, reactivation campaigns for At-Risk customers, and cost savings by suppressing low-value outreach to Hibernating customers.\"\n",
    "\n",
    "**Technical approach:**\n",
    "> \"I applied statistical validation methods from my analytical chemistry background—treating customer segments like analytical method validation with sensitivity analysis, hypothesis testing, and methodological comparison.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Key Metrics to Memorize:\n",
    "- **Dataset:** 540K transactions → 4,372 customers (after cleaning)\n",
    "- **Segments:** 5 (Champions, Loyal, Potential Loyalists, At Risk, Hibernating)\n",
    "- **Top segment:** Champions = 25% customers, 60% revenue\n",
    "- **Statistical validation:** ANOVA p < 0.001, ~70-80% RFM-to-k-means agreement\n",
    "- **Stability:** >80% quartile-to-quintile agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e27de",
   "metadata": {},
   "source": [
    "## 2. Technical Questions & Answers\n",
    "\n",
    "### Q1: \"Walk me through your approach to this project.\"\n",
    "\n",
    "**Answer structure (STAR format):**\n",
    "- **Situation:** E-commerce company needs customer segmentation for targeted marketing\n",
    "- **Task:** Segment customers by behavioral patterns using transactional data only\n",
    "- **Action:**\n",
    "  1. Data quality assessment (25% missing CustomerID, 9K cancellations)\n",
    "  2. RFM metric calculation (Recency, Frequency, Monetary)\n",
    "  3. Quartile-based scoring (1-4 scale, composite 3-12)\n",
    "  4. Statistical validation (ANOVA, k-means comparison, sensitivity analysis)\n",
    "  5. Business segmentation (5 actionable groups)\n",
    "- **Result:** 5 statistically distinct segments with clear business strategies\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: \"Why did you choose RFM over other clustering methods?\"\n",
    "\n",
    "**Answer:**\n",
    "1. **Interpretability:** Stakeholders understand \"recent\", \"frequent\", \"high-spending\" immediately\n",
    "2. **Business alignment:** Maps directly to marketing actions (retention, reactivation, upsell)\n",
    "3. **No feature engineering:** Works with raw transactional data\n",
    "4. **Industry standard:** Well-established methodology with literature support\n",
    "5. **Actionable:** Each segment has clear intervention strategy\n",
    "\n",
    "**But I validated it:** Compared RFM to k-means clustering (70-80% agreement) to ensure quartile boundaries weren't arbitrary.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: \"How did you handle missing data?\"\n",
    "\n",
    "**Answer:**\n",
    "- **Missing CustomerID (25%):** Removed—can't track behavior without customer identifier (likely guest checkouts)\n",
    "- **Cancellations (9K records):** Removed—returns would skew recency/frequency negatively\n",
    "- **Negative quantities/prices:** Removed—data entry errors\n",
    "\n",
    "**Trade-off acknowledged:** Introduces selection bias toward engaged customers, but ensures clean segmentation.\n",
    "\n",
    "**Why this approach?** Similar to analytical chemistry—better to have smaller, clean dataset than larger, noisy one.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: \"Why quartiles instead of quintiles or deciles?\"\n",
    "\n",
    "**Answer:**\n",
    "1. **Standard practice:** RFM literature typically uses quartiles\n",
    "2. **Balanced granularity:** 5 final segments balance detail vs. actionability\n",
    "3. **Tested robustness:** Ran sensitivity analysis—80%+ stability when switching to quintiles\n",
    "4. **Stakeholder comprehension:** Simpler to explain \"top 25%\" than \"top 10%\"\n",
    "\n",
    "**Key point:** I didn't just assume quartiles were right—I tested it empirically.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: \"Why qcut instead of cut for binning?\"\n",
    "\n",
    "**Answer:**\n",
    "- **Data-driven:** Distributions are highly right-skewed (shown in EDA)\n",
    "- **qcut (quantile-based):** Creates balanced bin sizes regardless of distribution\n",
    "- **cut (value-based):** Would result in 90% customers in one bin, 10% spread across others\n",
    "- **Practical example:** Without qcut, all customers spending <£500 would be in same bin, losing discrimination\n",
    "\n",
    "**Technical note:** Used `rank(method='first')` on Frequency to handle duplicate values.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6: \"How did you validate your segmentation?\"\n",
    "\n",
    "**Three-pronged approach:**\n",
    "\n",
    "1. **Statistical testing (ANOVA):**\n",
    "   - Null hypothesis: All segments have equal mean Monetary value\n",
    "   - Result: p < 0.001 → segments are statistically distinct\n",
    "   - Champions vs Hibernating: 10x spending difference (t-test p < 0.05)\n",
    "\n",
    "2. **Methodological comparison (k-means):**\n",
    "   - Compared rule-based RFM to data-driven k-means clustering\n",
    "   - 70-80% agreement between methods\n",
    "   - Conclusion: Quartile boundaries align with natural data structure\n",
    "\n",
    "3. **Sensitivity analysis (quartiles vs quintiles):**\n",
    "   - Tested robustness to parameter changes\n",
    "   - 80%+ stability in segment assignments\n",
    "   - Conclusion: Segmentation not overly sensitive to binning choice\n",
    "\n",
    "**Why this matters:** Demonstrates methodological rigor beyond \"I ran a clustering algorithm.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Q7: \"What would you do differently if you had more time/data?\"\n",
    "\n",
    "**Answer (shows forward thinking):**\n",
    "\n",
    "1. **Temporal dimension:** Track segment migration over time (cohort analysis)\n",
    "2. **Predictive modeling:** Build classifier to predict future segment (churn risk)\n",
    "3. **Product-level analysis:** Segment by product category affinity\n",
    "4. **External data:** Incorporate demographics, seasonality, marketing touchpoints\n",
    "5. **CLV refinement:** Use survival analysis instead of simple multiplicative model\n",
    "6. **A/B testing:** Validate business impact with controlled experiments\n",
    "\n",
    "---\n",
    "\n",
    "### Q8: \"Explain the business value of each segment.\"\n",
    "\n",
    "| Segment | % Customers | % Revenue | Strategy | ROI Expectation |\n",
    "|---------|------------|-----------|----------|----------------|\n",
    "| Champions | 25% | 60% | **Retention** (VIP programs, early access) | High - protect existing revenue |\n",
    "| Loyal Customers | 23% | 25% | **Upsell** (cross-sell, bundles) | Medium-High - incremental revenue |\n",
    "| Potential Loyalists | 18% | 10% | **Engagement** (onboarding, incentives) | Medium - conversion potential |\n",
    "| At Risk | 20% | 4% | **Reactivation** (win-back offers, surveys) | Low-Medium - save declining customers |\n",
    "| Hibernating | 14% | <1% | **Suppression** (reduce outreach costs) | Cost savings - not revenue |\n",
    "\n",
    "**Key insight:** 80/20 rule validated—top 48% customers drive 85% revenue.\n",
    "\n",
    "---\n",
    "\n",
    "### Q9: \"How would you deploy this in production?\"\n",
    "\n",
    "**Answer (shows system design thinking):**\n",
    "\n",
    "1. **Pipeline architecture:**\n",
    "   - ETL: Pull transactions from data warehouse (nightly batch)\n",
    "   - Calculate RFM metrics per customer\n",
    "   - Apply scoring logic (quartile boundaries from training data)\n",
    "   - Write segment assignments to CRM/marketing platform\n",
    "\n",
    "2. **Monitoring:**\n",
    "   - Track segment distribution over time (drift detection)\n",
    "   - Monitor key metrics: ANOVA p-value, avg revenue per segment\n",
    "   - Alert if >20% shift in segment composition\n",
    "\n",
    "3. **Retraining:**\n",
    "   - Quarterly: Recalculate quartile boundaries (data drift)\n",
    "   - Annually: Re-validate segment definitions (business changes)\n",
    "\n",
    "4. **Integration:**\n",
    "   - Export to Tableau/PowerBI for stakeholder dashboards\n",
    "   - API endpoint for real-time segment lookup\n",
    "   - CRM sync for automated campaign triggers\n",
    "\n",
    "---\n",
    "\n",
    "### Q10: \"What assumptions does RFM make? Are they valid?\"\n",
    "\n",
    "**Answer (shows critical thinking):**\n",
    "\n",
    "**Assumptions:**\n",
    "1. **Independence:** R, F, M treated as independent dimensions\n",
    "   - **Reality:** Frequency and Monetary are correlated (r ≈ 0.6)\n",
    "   - **Justification:** Acceptable—each dimension captures complementary behavior\n",
    "\n",
    "2. **Stationarity:** Customer behavior remains constant\n",
    "   - **Reality:** Customers migrate between segments\n",
    "   - **Mitigation:** Periodic recalculation (quarterly)\n",
    "\n",
    "3. **Equal weighting:** R, F, M contribute equally to score\n",
    "   - **Reality:** Business may value retention (R) over spending (M)\n",
    "   - **Alternative:** Could use weighted scoring based on business priorities\n",
    "\n",
    "4. **Linear scoring:** Quartile bins equally important\n",
    "   - **Reality:** Jump from Q3 to Q4 may be more significant than Q1 to Q2\n",
    "   - **Tested:** Sensitivity analysis showed robustness\n",
    "\n",
    "**Validity:** Assumptions are simplifications, not violations. Model is useful despite them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52fd74f",
   "metadata": {},
   "source": [
    "## 3. Methodology Deep Dive\n",
    "\n",
    "### RFM Calculation Details\n",
    "\n",
    "**Recency (R):**\n",
    "```python\n",
    "snapshot_date = df['InvoiceDate'].max() + timedelta(days=1)\n",
    "recency = (snapshot_date - customer_last_purchase_date).days\n",
    "```\n",
    "- **Why +1 day?** Makes analysis reproducible (not dependent on \"today\")\n",
    "- **Inverted scoring:** Recent = 4, Old = 1 (counterintuitive but correct)\n",
    "- **Range in data:** 1 to 374 days\n",
    "\n",
    "**Frequency (F):**\n",
    "```python\n",
    "frequency = df.groupby('CustomerID')['InvoiceNo'].nunique()\n",
    "```\n",
    "- **Why nunique?** Counts distinct orders, not line items\n",
    "- **Alternative considered:** Total items purchased (too granular)\n",
    "- **Range in data:** 1 to 210 orders\n",
    "\n",
    "**Monetary (M):**\n",
    "```python\n",
    "monetary = df.groupby('CustomerID')['TotalSpend'].sum()\n",
    "```\n",
    "- **Why sum?** Total customer lifetime value\n",
    "- **Alternative considered:** Average order value (loses scale information)\n",
    "- **Range in data:** £3.75 to £279,489\n",
    "\n",
    "---\n",
    "\n",
    "### Scoring Logic\n",
    "\n",
    "**Quartile Assignment:**\n",
    "```python\n",
    "r_labels = range(4, 0, -1)  # [4, 3, 2, 1] - INVERTED\n",
    "f_labels = range(1, 5)      # [1, 2, 3, 4]\n",
    "m_labels = range(1, 5)      # [1, 2, 3, 4]\n",
    "\n",
    "r_groups = pd.qcut(rfm['Recency'], q=4, labels=r_labels)\n",
    "```\n",
    "\n",
    "**Composite Score:**\n",
    "```python\n",
    "RFM_Score = R + F + M  # Range: 3 to 12\n",
    "```\n",
    "\n",
    "**Segment Mapping:**\n",
    "- 11-12: Champions\n",
    "- 9-10: Loyal Customers\n",
    "- 7-8: Potential Loyalists\n",
    "- 5-6: At Risk\n",
    "- 3-4: Hibernating\n",
    "\n",
    "**Why these thresholds?** Based on score distribution visualization—natural breakpoints observed.\n",
    "\n",
    "---\n",
    "\n",
    "### K-Means Comparison Details\n",
    "\n",
    "**Why compare to k-means?**\n",
    "- Validate that rule-based boundaries aren't arbitrary\n",
    "- Show quartile cutoffs align with natural data structure\n",
    "- Demonstrate methodological thinking (compare approaches)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Feature scaling required for distance-based algorithm\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])\n",
    "\n",
    "# 5 clusters to match RFM segment count\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "```\n",
    "\n",
    "**Evaluation:**\n",
    "- Silhouette score: 0.3-0.4 (acceptable separation)\n",
    "- Cross-tabulation: 70-80% agreement with RFM\n",
    "- Interpretation: Strong validation—both methods find similar structure\n",
    "\n",
    "---\n",
    "\n",
    "### ANOVA Technical Details\n",
    "\n",
    "**Test setup:**\n",
    "```python\n",
    "segments_list = [group['Monetary'].values for name, group in rfm.groupby('Segment')]\n",
    "f_stat, p_value = stats.f_oneway(*segments_list)\n",
    "```\n",
    "\n",
    "**Hypothesis:**\n",
    "- H₀: μ₁ = μ₂ = μ₃ = μ₄ = μ₅ (all segments have equal mean Monetary)\n",
    "- H₁: At least one mean is different\n",
    "\n",
    "**Result:**\n",
    "- F-statistic: ~2000+ (huge between-group variance)\n",
    "- p-value: < 0.001 (reject null hypothesis)\n",
    "- **Conclusion:** Segments are statistically distinct\n",
    "\n",
    "**Why ANOVA?**\n",
    "- Comparing >2 groups (can't use t-test)\n",
    "- Continuous outcome variable (Monetary)\n",
    "- Assumption: Normal distribution within groups (robust to violations with large n)\n",
    "\n",
    "---\n",
    "\n",
    "### Sensitivity Analysis Details\n",
    "\n",
    "**Purpose:** Test robustness to methodological choices\n",
    "\n",
    "**Approach:**\n",
    "1. Re-score using quintiles (5 bins) instead of quartiles (4 bins)\n",
    "2. Adjust segment thresholds for 3-15 range (vs. 3-12)\n",
    "3. Compare original vs. modified segment assignments\n",
    "4. Calculate stability: % customers with same segment label\n",
    "\n",
    "**Result:**\n",
    "- 80%+ stability (high)\n",
    "- Migrations mostly at segment boundaries (expected)\n",
    "- **Conclusion:** Segmentation robust to binning parameter changes\n",
    "\n",
    "**Why this matters:** Shows results aren't fragile/arbitrary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1543fd",
   "metadata": {},
   "source": [
    "## 4. Business Impact & Metrics\n",
    "\n",
    "### Key Business Metrics\n",
    "\n",
    "**Customer Distribution:**\n",
    "- Champions: 1,093 customers (25%)\n",
    "- Loyal: ~1,006 customers (23%)\n",
    "- Potential: ~787 customers (18%)\n",
    "- At Risk: ~875 customers (20%)\n",
    "- Hibernating: ~611 customers (14%)\n",
    "\n",
    "**Revenue Concentration:**\n",
    "- Top 25% (Champions): 60% of revenue\n",
    "- Top 48% (Champions + Loyal): 85% of revenue\n",
    "- Bottom 14% (Hibernating): <1% of revenue\n",
    "\n",
    "**Average Metrics by Segment:**\n",
    "- Champions: ~£5,000 spend, ~100 orders, ~20 days recency\n",
    "- Hibernating: ~£300 spend, ~1-2 orders, ~300 days recency\n",
    "\n",
    "---\n",
    "\n",
    "### Actionable Insights\n",
    "\n",
    "**1. Retention Priority (Champions):**\n",
    "- Risk: Losing 25% customers = 60% revenue loss\n",
    "- Action: VIP program, dedicated support, early product access\n",
    "- Metric: Retention rate (target: >95%)\n",
    "\n",
    "**2. Growth Opportunity (Potential Loyalists):**\n",
    "- Upside: 18% customers contributing only 10% revenue\n",
    "- Action: Personalized onboarding, purchase incentives\n",
    "- Metric: Conversion to Loyal (target: 30% within 6 months)\n",
    "\n",
    "**3. Cost Optimization (Hibernating):**\n",
    "- Problem: 14% customers, <1% revenue, receiving same marketing\n",
    "- Action: Suppress non-targeted outreach, save marketing budget\n",
    "- Metric: Cost savings (est: 10-15% marketing budget)\n",
    "\n",
    "**4. Win-back Campaign (At Risk):**\n",
    "- Insight: 20% customers showing declining engagement\n",
    "- Action: Survey (why leaving?), special offers, re-engagement emails\n",
    "- Metric: Reactivation rate (target: 15-20%)\n",
    "\n",
    "---\n",
    "\n",
    "### ROI Estimation\n",
    "\n",
    "**Scenario: Retention Campaign for Champions**\n",
    "- Current Champions: 1,093 customers\n",
    "- Avg annual value: £5,000 × (100 orders / 13 months) × 12 = £46,000\n",
    "- If we lose 5% without intervention: 55 customers × £46,000 = £2.5M loss\n",
    "- VIP program cost: £200/customer/year = £218K\n",
    "- **ROI:** Prevent £2.5M loss for £218K investment = 11x return\n",
    "\n",
    "**Scenario: Cost Savings from Hibernating Suppression**\n",
    "- Hibernating customers: 611\n",
    "- Current marketing cost: £50/customer/year = £30,550\n",
    "- Revenue from this group: <£10K/year\n",
    "- **Savings:** £20K+/year by reducing outreach frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498aca8",
   "metadata": {},
   "source": [
    "## 5. Challenges & Solutions\n",
    "\n",
    "### Challenge 1: Highly Skewed Distributions\n",
    "\n",
    "**Problem:**\n",
    "- Frequency: Mean = 90, Median = 3 (extreme right skew)\n",
    "- Monetary: Mean = £1,900, Median = £350\n",
    "- Few high-value customers dominate\n",
    "\n",
    "**Solution:**\n",
    "1. Used qcut (quantile-based) instead of cut (value-based)\n",
    "2. Tested log transformation (decided against—interpretability loss)\n",
    "3. Visualized distributions before binning\n",
    "\n",
    "**Why it worked:** qcut creates balanced bins regardless of distribution shape\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 2: Frequency Duplicate Values\n",
    "\n",
    "**Problem:**\n",
    "- Many customers have same Frequency (1 order, 2 orders, etc.)\n",
    "- qcut fails when bin edges have duplicate values\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "f_groups = pd.qcut(rfm['Frequency'].rank(method='first'), q=4, labels=f_labels)\n",
    "```\n",
    "- `rank(method='first')` breaks ties by maintaining order\n",
    "- Creates unique ranking for qcut to work on\n",
    "\n",
    "**Why it worked:** Preserves relative ordering while handling duplicates\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 3: Correlated Variables (F and M)\n",
    "\n",
    "**Problem:**\n",
    "- Frequency and Monetary are correlated (r ≈ 0.6)\n",
    "- Violates RFM independence assumption\n",
    "- Risk: Double-counting similar information\n",
    "\n",
    "**Solution:**\n",
    "1. Acknowledged the correlation explicitly\n",
    "2. Justified as acceptable (each dimension captures distinct behavior)\n",
    "3. Considered PCA but rejected (interpretability loss)\n",
    "4. Validated with k-means (confirms segments are meaningful)\n",
    "\n",
    "**Why acceptable:** Business needs interpretable dimensions, not orthogonal ones\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 4: Arbitrary Segment Thresholds\n",
    "\n",
    "**Problem:**\n",
    "- Why is score 11-12 \"Champions\" vs. 10-12?\n",
    "- Thresholds seem subjective\n",
    "\n",
    "**Solution:**\n",
    "1. Visualized score distribution (histogram)\n",
    "2. Identified natural breakpoints\n",
    "3. Validated with sensitivity analysis\n",
    "4. Compared to k-means (data-driven approach)\n",
    "\n",
    "**Result:** 80%+ stability across different thresholds\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 5: Missing CustomerID (25% of data)\n",
    "\n",
    "**Problem:**\n",
    "- Can't perform customer-level analysis without ID\n",
    "- Losing significant portion of data\n",
    "\n",
    "**Solution:**\n",
    "1. Removed records without CustomerID\n",
    "2. Acknowledged selection bias (engaged customers only)\n",
    "3. Documented decision and trade-off\n",
    "4. Suggested future work: guest checkout analysis\n",
    "\n",
    "**Why acceptable:** Clean segmentation > noisy large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d7c00",
   "metadata": {},
   "source": [
    "## 6. Extension Ideas (Future Work)\n",
    "\n",
    "### 1. Temporal Analysis\n",
    "\n",
    "**Goal:** Track segment migration over time\n",
    "\n",
    "**Approach:**\n",
    "- Calculate RFM scores monthly (rolling window)\n",
    "- Create transition matrix: P(moving from segment A to segment B)\n",
    "- Identify churn signals: Champions → At Risk pattern\n",
    "\n",
    "**Business value:** Early warning system for customer churn\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Predictive Modeling\n",
    "\n",
    "**Goal:** Predict future segment assignment\n",
    "\n",
    "**Approach:**\n",
    "- Features: Current RFM scores, trend (3-month change), seasonality\n",
    "- Target: Segment 3 months in future\n",
    "- Model: Random Forest or Gradient Boosting\n",
    "\n",
    "**Business value:** Proactive interventions before customers decline\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Product-Level Segmentation\n",
    "\n",
    "**Goal:** Understand product affinity by segment\n",
    "\n",
    "**Approach:**\n",
    "- Market basket analysis within each segment\n",
    "- Identify category preferences (Champions buy X, Hibernating buy Y)\n",
    "- Collaborative filtering for recommendations\n",
    "\n",
    "**Business value:** Targeted product recommendations per segment\n",
    "\n",
    "---\n",
    "\n",
    "### 4. CLV Refinement\n",
    "\n",
    "**Goal:** More accurate lifetime value estimation\n",
    "\n",
    "**Approach:**\n",
    "- Survival analysis (time-to-churn modeling)\n",
    "- Cohort-based retention curves\n",
    "- Discount future cash flows (WACC)\n",
    "- Account for reactivation probability\n",
    "\n",
    "**Business value:** Better investment decisions (acquisition cost vs. CLV)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. A/B Testing Framework\n",
    "\n",
    "**Goal:** Validate business impact empirically\n",
    "\n",
    "**Approach:**\n",
    "- Treatment: Champions receive VIP program\n",
    "- Control: Champions receive standard marketing\n",
    "- Metrics: Retention rate, revenue per customer, NPS\n",
    "- Duration: 6 months\n",
    "\n",
    "**Business value:** Prove ROI of segmentation-based strategies\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Multi-Channel Integration\n",
    "\n",
    "**Goal:** Incorporate channel preferences\n",
    "\n",
    "**Approach:**\n",
    "- Track engagement by channel (email, SMS, push, direct mail)\n",
    "- Identify optimal channel per segment\n",
    "- Test channel-switching experiments\n",
    "\n",
    "**Business value:** Improve campaign efficiency (right message, right channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a19f98",
   "metadata": {},
   "source": [
    "## 7. Code Walkthroughs (Key Snippets)\n",
    "\n",
    "### Snippet 1: RFM Calculation (Core Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# INTERVIEWER QUESTION: \"Walk me through this code\"\n",
    "# ANSWER: \"This aggregates transactional data to customer-level metrics\"\n",
    "\n",
    "# Reference date for recency calculation\n",
    "snapshot_date = df['InvoiceDate'].max() + dt.timedelta(days=1)\n",
    "# WHY +1? Makes analysis reproducible (not dependent on \"today\")\n",
    "\n",
    "# Group by customer and calculate metrics\n",
    "rfm = df.groupby(['CustomerID']).agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,  # Days since last order\n",
    "    'InvoiceNo': 'nunique',                                    # Number of unique orders\n",
    "    'TotalSpend': 'sum'                                        # Total lifetime spend\n",
    "}).reset_index()\n",
    "\n",
    "# Rename for clarity\n",
    "rfm.rename(columns={\n",
    "    'InvoiceDate': 'Recency',\n",
    "    'InvoiceNo': 'Frequency',\n",
    "    'TotalSpend': 'Monetary'\n",
    "}, inplace=True)\n",
    "\n",
    "print(rfm.head())\n",
    "# EXPECTED QUESTION: \"Why lambda for InvoiceDate but string for others?\"\n",
    "# ANSWER: \"Custom calculation (date difference) vs. built-in aggregations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e56e2",
   "metadata": {},
   "source": [
    "### Snippet 2: Quartile Scoring with qcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERVIEWER QUESTION: \"Why qcut instead of cut?\"\n",
    "# ANSWER: \"Data is right-skewed. Cut would create unbalanced bins.\"\n",
    "\n",
    "# Define labels\n",
    "r_labels = range(4, 0, -1)  # [4, 3, 2, 1] - INVERTED for recency\n",
    "f_labels = range(1, 5)      # [1, 2, 3, 4]\n",
    "m_labels = range(1, 5)      # [1, 2, 3, 4]\n",
    "\n",
    "# Apply quantile-based binning\n",
    "r_groups = pd.qcut(rfm['Recency'], q=4, labels=r_labels, duplicates='drop')\n",
    "# WHY rank()? Handles duplicate frequency values\n",
    "f_groups = pd.qcut(rfm['Frequency'].rank(method='first'), q=4, labels=f_labels, duplicates='drop')\n",
    "m_groups = pd.qcut(rfm['Monetary'], q=4, labels=m_labels, duplicates='drop')\n",
    "\n",
    "# Add to dataframe\n",
    "rfm['R'] = r_groups.values.astype(int)\n",
    "rfm['F'] = f_groups.values.astype(int)\n",
    "rfm['M'] = m_groups.values.astype(int)\n",
    "\n",
    "# Calculate composite score\n",
    "rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1)\n",
    "\n",
    "# EXPECTED QUESTION: \"What's the score range?\"\n",
    "# ANSWER: \"3 to 12. Min: (1+1+1), Max: (4+4+4)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eab9ac",
   "metadata": {},
   "source": [
    "### Snippet 3: ANOVA Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# INTERVIEWER QUESTION: \"How did you validate the segmentation?\"\n",
    "# ANSWER: \"ANOVA to test if segments have statistically different means\"\n",
    "\n",
    "# Prepare data for ANOVA (list of arrays, one per segment)\n",
    "segments_list = [group['Monetary'].values for name, group in rfm.groupby('Segment')]\n",
    "\n",
    "# Run one-way ANOVA\n",
    "f_stat, p_value = stats.f_oneway(*segments_list)\n",
    "\n",
    "print(f\"F-statistic: {f_stat:.2f}\")\n",
    "print(f\"P-value: {p_value:.2e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Segments are statistically distinct\")\n",
    "    \n",
    "# EXPECTED QUESTION: \"What does the F-statistic mean?\"\n",
    "# ANSWER: \"Ratio of between-group variance to within-group variance.\n",
    "#          High F = segments are well-separated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b6eb5",
   "metadata": {},
   "source": [
    "### Snippet 4: K-Means Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d195d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# INTERVIEWER QUESTION: \"Why compare to k-means?\"\n",
    "# ANSWER: \"To validate RFM quartile boundaries align with natural data structure\"\n",
    "\n",
    "# Feature scaling (required for distance-based algorithms)\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])\n",
    "\n",
    "# Fit k-means with 5 clusters (match RFM segment count)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "rfm['KMeans_Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Calculate silhouette score (cluster quality metric)\n",
    "silhouette = silhouette_score(rfm_scaled, rfm['KMeans_Cluster'])\n",
    "print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "\n",
    "# Compare assignments\n",
    "comparison = pd.crosstab(rfm['Segment'], rfm['KMeans_Cluster'])\n",
    "print(comparison)\n",
    "\n",
    "# EXPECTED QUESTION: \"What's a good silhouette score?\"\n",
    "# ANSWER: \"0.3-0.4 is acceptable. >0.5 is good. <0.2 suggests poor separation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3455a",
   "metadata": {},
   "source": [
    "### Snippet 5: Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b752c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERVIEWER QUESTION: \"How do you know your results are robust?\"\n",
    "# ANSWER: \"Sensitivity analysis - tested quartiles vs quintiles\"\n",
    "\n",
    "# Re-score using quintiles (5 bins instead of 4)\n",
    "r_labels_quint = range(5, 0, -1)\n",
    "f_labels_quint = range(1, 6)\n",
    "m_labels_quint = range(1, 6)\n",
    "\n",
    "r_groups_quint = pd.qcut(rfm['Recency'], q=5, labels=r_labels_quint, duplicates='drop')\n",
    "f_groups_quint = pd.qcut(rfm['Frequency'].rank(method='first'), q=5, labels=f_labels_quint, duplicates='drop')\n",
    "m_groups_quint = pd.qcut(rfm['Monetary'], q=5, labels=m_labels_quint, duplicates='drop')\n",
    "\n",
    "rfm['RFM_Score_quint'] = (r_groups_quint.astype(int) + \n",
    "                          f_groups_quint.astype(int) + \n",
    "                          m_groups_quint.astype(int))\n",
    "\n",
    "# Assign segments with adjusted thresholds (3-15 range now)\n",
    "def segment_customer_quintile(score):\n",
    "    if score >= 13: return 'Champions'\n",
    "    elif score >= 11: return 'Loyal Customers'\n",
    "    elif score >= 9: return 'Potential Loyalists'\n",
    "    elif score >= 7: return 'At Risk'\n",
    "    else: return 'Hibernating'\n",
    "\n",
    "rfm['Segment_Quintile'] = rfm['RFM_Score_quint'].apply(segment_customer_quintile)\n",
    "\n",
    "# Calculate stability\n",
    "stability = (rfm['Segment'] == rfm['Segment_Quintile']).mean()\n",
    "print(f\"Stability: {stability*100:.1f}%\")\n",
    "\n",
    "# EXPECTED QUESTION: \"What's acceptable stability?\"\n",
    "# ANSWER: \">80% is high. 60-80% is moderate. <60% suggests fragile segmentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fd275",
   "metadata": {},
   "source": [
    "## Practice Questions for Live Coding\n",
    "\n",
    "### Question 1: \"Calculate average order value by segment\"\n",
    "\n",
    "```python\n",
    "# Expected answer:\n",
    "rfm['Avg_Order_Value'] = rfm['Monetary'] / rfm['Frequency']\n",
    "aov_by_segment = rfm.groupby('Segment')['Avg_Order_Value'].mean()\n",
    "print(aov_by_segment)\n",
    "```\n",
    "\n",
    "### Question 2: \"Find customers who moved from Champions to At Risk\"\n",
    "\n",
    "```python\n",
    "# Expected answer:\n",
    "churning_customers = rfm[(rfm['Segment_Previous'] == 'Champions') & \n",
    "                         (rfm['Segment'] == 'At Risk')]\n",
    "print(f\"Churning champions: {len(churning_customers)}\")\n",
    "```\n",
    "\n",
    "### Question 3: \"What percentage of revenue comes from top 10% customers?\"\n",
    "\n",
    "```python\n",
    "# Expected answer:\n",
    "rfm_sorted = rfm.sort_values('Monetary', ascending=False)\n",
    "top_10_pct = rfm_sorted.head(int(len(rfm) * 0.1))\n",
    "revenue_concentration = (top_10_pct['Monetary'].sum() / rfm['Monetary'].sum()) * 100\n",
    "print(f\"Top 10% customers: {revenue_concentration:.1f}% of revenue\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb350a",
   "metadata": {},
   "source": [
    "## Final Interview Tips\n",
    "\n",
    "### Do's:\n",
    "1. ✅ Start with business context, then dive into technical\n",
    "2. ✅ Use STAR format for behavioral questions\n",
    "3. ✅ Draw diagrams (pipeline, segment distribution)\n",
    "4. ✅ Acknowledge assumptions and limitations\n",
    "5. ✅ Show you validated your approach (ANOVA, k-means, sensitivity)\n",
    "6. ✅ Connect to business impact (revenue, retention, cost)\n",
    "7. ✅ Mention your analytical chemistry background (method validation)\n",
    "8. ✅ Prepare 2-3 follow-up questions for interviewer\n",
    "\n",
    "### Don'ts:\n",
    "1. ❌ Don't jump straight to code without context\n",
    "2. ❌ Don't say \"I just ran the algorithm\"\n",
    "3. ❌ Don't ignore data quality issues\n",
    "4. ❌ Don't claim results without validation\n",
    "5. ❌ Don't use jargon without explaining\n",
    "6. ❌ Don't say \"it's obvious\" or \"everyone knows\"\n",
    "\n",
    "### Key Phrases to Use:\n",
    "- \"To validate this approach, I compared...\"\n",
    "- \"The trade-off here is...\"\n",
    "- \"From a business perspective...\"\n",
    "- \"I tested robustness by...\"\n",
    "- \"The assumption is... which is acceptable because...\"\n",
    "- \"Drawing from my research background...\"\n",
    "\n",
    "### Red Flags to Avoid:\n",
    "- \"I used this method because it's popular\"\n",
    "- \"The results look good\" (without metrics)\n",
    "- \"I didn't check for [basic issue]\"\n",
    "- \"I copied this code from...\"\n",
    "- \"I'm not sure why it works\"\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference Card (Memorize This)\n",
    "\n",
    "**Project Stats:**\n",
    "- 540K → 4,372 customers\n",
    "- 5 segments, Champions = 25%, 60% revenue\n",
    "- ANOVA p < 0.001\n",
    "- 70-80% RFM-k-means agreement\n",
    "- 80%+ quartile-quintile stability\n",
    "\n",
    "**Key Methods:**\n",
    "- qcut (not cut) for skewed data\n",
    "- rank(method='first') for duplicate handling\n",
    "- StandardScaler for k-means\n",
    "- f_oneway for ANOVA\n",
    "- Silhouette score for cluster quality\n",
    "\n",
    "**Business Impact:**\n",
    "- Champions: Retain (60% revenue)\n",
    "- Loyal: Upsell (25% revenue)\n",
    "- Potential: Convert (10% revenue)\n",
    "- At Risk: Reactivate (4% revenue)\n",
    "- Hibernating: Suppress (<1% revenue)\n",
    "\n",
    "**Your Differentiator:**\n",
    "\"I treated customer segmentation like analytical method validation—testing assumptions, comparing approaches, and quantifying uncertainty. That's the rigor I bring from 16 years in analytical chemistry research.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55383bd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Theoretical Foundations & Command Explanations\n",
    "\n",
    "### Statistical Theories\n",
    "\n",
    "#### 8.1 One-Way ANOVA (Analysis of Variance)\n",
    "\n",
    "**Theory:**\n",
    "ANOVA tests whether the means of multiple groups are statistically different. It compares the variance **between** groups to the variance **within** groups.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "$$F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}} = \\frac{MS_{between}}{MS_{within}}$$\n",
    "\n",
    "Where:\n",
    "- $MS_{between} = \\frac{SS_{between}}{df_{between}}$ (Mean Square Between)\n",
    "- $MS_{within} = \\frac{SS_{within}}{df_{within}}$ (Mean Square Within)\n",
    "- $SS = \\sum(x_i - \\bar{x})^2$ (Sum of Squares)\n",
    "\n",
    "**Hypothesis:**\n",
    "- $H_0$: $\\mu_1 = \\mu_2 = ... = \\mu_k$ (all group means are equal)\n",
    "- $H_1$: At least one mean is different\n",
    "\n",
    "**Assumptions:**\n",
    "1. Independence: Observations are independent\n",
    "2. Normality: Data within each group is normally distributed\n",
    "3. Homogeneity of variance: Equal variance across groups (Levene's test)\n",
    "\n",
    "**In Our Project:**\n",
    "- Groups: 5 customer segments\n",
    "- Variable: Monetary value\n",
    "- Result: F-statistic ≈ 2000+, p < 0.001\n",
    "- Interpretation: Segment means are significantly different\n",
    "\n",
    "**Why ANOVA and not t-tests?**\n",
    "- Multiple t-tests increase Type I error (false positives)\n",
    "- With 5 groups, we'd need 10 pairwise t-tests\n",
    "- ANOVA controls family-wise error rate\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.2 Independent Samples T-Test\n",
    "\n",
    "**Theory:**\n",
    "Tests whether two independent groups have different means.\n",
    "\n",
    "**Formula:**\n",
    "$$t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$$\n",
    "\n",
    "Where:\n",
    "- $\\bar{x}$ = sample means\n",
    "- $s^2$ = sample variances\n",
    "- $n$ = sample sizes\n",
    "\n",
    "**In Our Project:**\n",
    "- Compared Champions vs Hibernating\n",
    "- Result: t ≈ 50+, p < 0.001\n",
    "- Interpretation: Champions spend significantly more (10x difference)\n",
    "\n",
    "**Assumptions:**\n",
    "1. Independence between groups\n",
    "2. Normal distribution (robust with large n due to CLT)\n",
    "3. Homogeneity of variance (can use Welch's t-test if violated)\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.3 Pearson Correlation Coefficient\n",
    "\n",
    "**Theory:**\n",
    "Measures linear relationship strength between two continuous variables.\n",
    "\n",
    "**Formula:**\n",
    "$$r = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "**Range:** -1 to +1\n",
    "- r = +1: Perfect positive correlation\n",
    "- r = 0: No linear correlation\n",
    "- r = -1: Perfect negative correlation\n",
    "\n",
    "**In Our Project:**\n",
    "- Frequency vs Monetary: r ≈ 0.6 (moderate positive)\n",
    "- Recency vs Frequency: r ≈ -0.3 (weak negative)\n",
    "- Interpretation: Frequent buyers tend to spend more (expected)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Tests RFM independence assumption\n",
    "- Correlation ≠ redundancy if dimensions capture different behaviors\n",
    "\n",
    "---\n",
    "\n",
    "### Machine Learning Concepts\n",
    "\n",
    "#### 8.4 K-Means Clustering\n",
    "\n",
    "**Theory:**\n",
    "Unsupervised algorithm that partitions data into k clusters by minimizing within-cluster variance.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize k cluster centers (random or k-means++)\n",
    "2. Assign each point to nearest center (Euclidean distance)\n",
    "3. Recalculate centers as mean of assigned points\n",
    "4. Repeat steps 2-3 until convergence\n",
    "\n",
    "**Objective Function (minimize):**\n",
    "$$J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "\n",
    "Where:\n",
    "- $C_i$ = cluster i\n",
    "- $\\mu_i$ = centroid of cluster i\n",
    "- $||·||$ = Euclidean distance\n",
    "\n",
    "**In Our Project:**\n",
    "- k = 5 (to match RFM segment count)\n",
    "- Features: Recency, Frequency, Monetary (scaled)\n",
    "- Purpose: Validate RFM quartile boundaries\n",
    "\n",
    "**Why Feature Scaling?**\n",
    "K-means uses Euclidean distance. Without scaling:\n",
    "- Monetary (£1000s) dominates distance calculation\n",
    "- Recency (days) and Frequency (counts) have minimal impact\n",
    "- StandardScaler transforms to mean=0, std=1\n",
    "\n",
    "**Limitations:**\n",
    "1. Assumes spherical clusters\n",
    "2. Sensitive to initialization (use multiple n_init)\n",
    "3. Must specify k in advance\n",
    "4. Not robust to outliers\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.5 Silhouette Score\n",
    "\n",
    "**Theory:**\n",
    "Measures how similar an object is to its own cluster compared to other clusters.\n",
    "\n",
    "**Formula (per sample):**\n",
    "$$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n",
    "\n",
    "Where:\n",
    "- $a(i)$ = average distance to points in same cluster\n",
    "- $b(i)$ = average distance to points in nearest different cluster\n",
    "\n",
    "**Range:** -1 to +1\n",
    "- s > 0.7: Strong structure\n",
    "- 0.5 < s < 0.7: Reasonable structure\n",
    "- 0.25 < s < 0.5: Weak structure (overlapping clusters)\n",
    "- s < 0.25: Poor structure\n",
    "\n",
    "**In Our Project:**\n",
    "- Score ≈ 0.3-0.4\n",
    "- Interpretation: Acceptable cluster separation\n",
    "- Validates that segments have distinct characteristics\n",
    "\n",
    "**Average Silhouette:**\n",
    "$$\\bar{s} = \\frac{1}{n} \\sum_{i=1}^{n} s(i)$$\n",
    "\n",
    "---\n",
    "\n",
    "### Pandas/NumPy Commands\n",
    "\n",
    "#### 8.6 GroupBy-Aggregate Pattern\n",
    "\n",
    "**Theory:**\n",
    "Split-apply-combine paradigm for data aggregation.\n",
    "\n",
    "**Command:**\n",
    "```python\n",
    "rfm = df.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'nunique',\n",
    "    'TotalSpend': 'sum'\n",
    "})\n",
    "```\n",
    "\n",
    "**What Happens:**\n",
    "1. **Split:** Group rows by CustomerID\n",
    "2. **Apply:** Execute aggregation function on each group\n",
    "3. **Combine:** Merge results into new DataFrame\n",
    "\n",
    "**Aggregation Functions:**\n",
    "- `'sum'`, `'mean'`, `'count'`: Built-in string functions\n",
    "- `lambda x: ...`: Custom functions\n",
    "- `'nunique'`: Count distinct values\n",
    "- `['min', 'max']`: Multiple functions\n",
    "\n",
    "**Why Lambda for InvoiceDate?**\n",
    "- Need custom calculation: `(snapshot_date - max_date).days`\n",
    "- Built-in functions don't support date arithmetic\n",
    "- Lambda receives Series, returns single value\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.7 pd.qcut vs pd.cut\n",
    "\n",
    "**Theory:**\n",
    "Two approaches to binning continuous variables.\n",
    "\n",
    "**pd.cut (Value-based binning):**\n",
    "```python\n",
    "pd.cut(x, bins=[0, 100, 500, 1000, 5000])\n",
    "```\n",
    "- Divides range into equal-width intervals\n",
    "- Bins based on **values**, not distribution\n",
    "- Example: [0-100], [100-500], [500-1000], [1000-5000]\n",
    "\n",
    "**pd.qcut (Quantile-based binning):**\n",
    "```python\n",
    "pd.qcut(x, q=4)  # Quartiles\n",
    "```\n",
    "- Divides into equal-frequency intervals\n",
    "- Bins based on **percentiles**, not values\n",
    "- Example: Each bin contains 25% of data\n",
    "\n",
    "**When to Use Which?**\n",
    "\n",
    "| Scenario | Use | Reason |\n",
    "|----------|-----|--------|\n",
    "| Normally distributed data | `cut` | Value-based makes sense |\n",
    "| Skewed distribution | `qcut` | Prevents unbalanced bins |\n",
    "| Known meaningful thresholds | `cut` | E.g., income brackets |\n",
    "| Unknown distribution | `qcut` | Data-driven approach |\n",
    "\n",
    "**In Our Project:**\n",
    "- Data is highly right-skewed (Monetary: mean £1,900, median £350)\n",
    "- Using `cut` would put 90% customers in lowest bin\n",
    "- `qcut` ensures balanced bins (25% per quartile)\n",
    "\n",
    "**Mathematical Basis:**\n",
    "- Quartile: $Q_k = x_{n \\cdot k/4}$ (value at k/4 position in sorted data)\n",
    "- Percentile: $P_k = x_{n \\cdot k/100}$\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.8 rank(method='first')\n",
    "\n",
    "**Theory:**\n",
    "Handles duplicate values when binning.\n",
    "\n",
    "**Problem:**\n",
    "```python\n",
    "df = pd.DataFrame({'freq': [1, 1, 1, 2, 2, 3, 4, 5]})\n",
    "pd.qcut(df['freq'], q=4)  # Error! Bin edges have duplicates\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "pd.qcut(df['freq'].rank(method='first'), q=4)\n",
    "```\n",
    "\n",
    "**Ranking Methods:**\n",
    "\n",
    "| Method | Behavior | Example: [1, 1, 1, 2] |\n",
    "|--------|----------|----------------------|\n",
    "| `'average'` | Assign average rank | [2.0, 2.0, 2.0, 4.0] |\n",
    "| `'min'` | Assign lowest rank | [1, 1, 1, 4] |\n",
    "| `'max'` | Assign highest rank | [3, 3, 3, 4] |\n",
    "| `'first'` | Assign by order | [1, 2, 3, 4] |\n",
    "| `'dense'` | Consecutive ranks | [1, 1, 1, 2] |\n",
    "\n",
    "**Why 'first'?**\n",
    "- Creates unique ranking for qcut to work\n",
    "- Preserves relative ordering\n",
    "- Arbitrary but consistent tie-breaking\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.9 StandardScaler\n",
    "\n",
    "**Theory:**\n",
    "Transforms features to have mean=0 and standard deviation=1.\n",
    "\n",
    "**Formula:**\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $x$ = original value\n",
    "- $\\mu$ = feature mean\n",
    "- $\\sigma$ = feature standard deviation\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = [[1000, 10], [2000, 20], [3000, 30]]  # [Monetary, Frequency]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Result: [[−1.22, −1.22], [0.00, 0.00], [1.22, 1.22]]\n",
    "```\n",
    "\n",
    "**Why Scale?**\n",
    "Distance-based algorithms (k-means, KNN) are sensitive to feature magnitude:\n",
    "- Without scaling: Distance dominated by large-scale features\n",
    "- With scaling: All features contribute equally\n",
    "\n",
    "**Alternative Scalers:**\n",
    "- **MinMaxScaler:** $(x - \\min) / (\\max - \\min)$ → [0, 1]\n",
    "- **RobustScaler:** Uses median and IQR (robust to outliers)\n",
    "- **Normalizer:** Scales each sample to unit norm\n",
    "\n",
    "**In Our Project:**\n",
    "- Monetary: £3 to £279,489 (huge range)\n",
    "- Frequency: 1 to 210 (smaller range)\n",
    "- Recency: 1 to 374 (medium range)\n",
    "- Without scaling, Monetary would dominate k-means\n",
    "\n",
    "---\n",
    "\n",
    "### RFM-Specific Theory\n",
    "\n",
    "#### 8.10 Why Inverted Recency Scoring?\n",
    "\n",
    "**Conceptual Issue:**\n",
    "- Low recency (recent purchase) = Good customer\n",
    "- But in scoring, we want high = good\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "r_labels = range(4, 0, -1)  # [4, 3, 2, 1] instead of [1, 2, 3, 4]\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "| Days Since Purchase | Quartile | Standard Score | Inverted Score |\n",
    "|---------------------|----------|----------------|----------------|\n",
    "| 10 (recent) | Q1 | 1 | **4** ✓ |\n",
    "| 100 | Q2 | 2 | **3** |\n",
    "| 200 | Q3 | 3 | **2** |\n",
    "| 350 (old) | Q4 | 4 | **1** ✓ |\n",
    "\n",
    "**Why This Matters:**\n",
    "- Composite RFM score: R + F + M\n",
    "- All three should be \"higher = better\"\n",
    "- Without inversion: Recent customer gets low score (wrong!)\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.11 Composite Score vs. Multi-dimensional Segmentation\n",
    "\n",
    "**Approach 1: Composite Score (Used)**\n",
    "```python\n",
    "RFM_Score = R + F + M  # Single number: 3-12\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simple to explain\n",
    "- Easy to rank customers\n",
    "- Natural segments emerge\n",
    "\n",
    "**Cons:**\n",
    "- Equal weighting assumption\n",
    "- Loses granularity (e.g., 4-4-4 = 3-5-4 = 12)\n",
    "\n",
    "**Approach 2: Multi-dimensional (Alternative)**\n",
    "```python\n",
    "# Keep R, F, M separate\n",
    "# Segment by: (R, F, M) tuples\n",
    "# E.g., (4, 4, 4) = \"Champions\", (4, 1, 1) = \"New Customers\"\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Preserves full information\n",
    "- More nuanced segments\n",
    "\n",
    "**Cons:**\n",
    "- 4³ = 64 possible combinations (too many!)\n",
    "- Hard to interpret\n",
    "\n",
    "**Approach 3: Weighted Score**\n",
    "```python\n",
    "RFM_Score = 0.5*R + 0.3*F + 0.2*M  # Custom weights\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- Business prioritizes retention (R) over spending (M)\n",
    "- Data-driven: Optimize weights for prediction task\n",
    "\n",
    "---\n",
    "\n",
    "### Data Preprocessing Theory\n",
    "\n",
    "#### 8.12 Handling Missing Data\n",
    "\n",
    "**Missing Mechanism Types:**\n",
    "\n",
    "1. **MCAR (Missing Completely at Random):**\n",
    "   - Missing values independent of both observed and unobserved data\n",
    "   - Example: Sensor failure (random)\n",
    "   - Strategy: Deletion or imputation both work\n",
    "\n",
    "2. **MAR (Missing at Random):**\n",
    "   - Missing values related to observed data, not unobserved\n",
    "   - Example: Younger customers skip age field more often\n",
    "   - Strategy: Imputation with related variables\n",
    "\n",
    "3. **MNAR (Missing Not at Random):**\n",
    "   - Missing values related to unobserved data itself\n",
    "   - Example: High earners don't report income\n",
    "   - Strategy: Model missingness mechanism\n",
    "\n",
    "**In Our Project:**\n",
    "- Missing CustomerID: Likely MNAR (guest checkout by choice)\n",
    "- Strategy: Deletion (can't impute customer identity)\n",
    "- Trade-off: Lose 25% data but ensure validity\n",
    "\n",
    "**Imputation Methods:**\n",
    "- **Mean/Median:** Simple but ignores relationships\n",
    "- **KNN:** Uses similar observations\n",
    "- **Regression:** Predicts missing from other features\n",
    "- **Multiple Imputation:** Accounts for uncertainty\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.13 Outlier Detection & Treatment\n",
    "\n",
    "**Detection Methods:**\n",
    "\n",
    "1. **IQR Method:**\n",
    "   - Outlier if: $x < Q_1 - 1.5 \\times IQR$ or $x > Q_3 + 1.5 \\times IQR$\n",
    "   - Where: $IQR = Q_3 - Q_1$\n",
    "\n",
    "2. **Z-Score Method:**\n",
    "   - Outlier if: $|z| > 3$ where $z = (x - \\mu) / \\sigma$\n",
    "\n",
    "3. **Isolation Forest:**\n",
    "   - ML-based anomaly detection\n",
    "\n",
    "**Treatment Strategies:**\n",
    "\n",
    "| Strategy | When to Use | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Remove** | Data errors | Clean dataset | Lose information |\n",
    "| **Cap (winsorize)** | Valid but extreme | Keeps all data | Arbitrary cutoff |\n",
    "| **Transform (log)** | Skewed distribution | Normalizes | Changes scale |\n",
    "| **Keep** | Genuine variation | Preserves reality | Affects statistics |\n",
    "\n",
    "**In Our Project:**\n",
    "- Detected via box plots\n",
    "- Strategy: Keep outliers (legitimate high-value customers)\n",
    "- Mitigation: Used qcut (robust to outliers)\n",
    "\n",
    "---\n",
    "\n",
    "### Statistical Concepts for Interviews\n",
    "\n",
    "#### 8.14 Type I vs Type II Errors\n",
    "\n",
    "**Definitions:**\n",
    "- **Type I Error (α):** False Positive - Reject true null hypothesis\n",
    "- **Type II Error (β):** False Negative - Fail to reject false null hypothesis\n",
    "\n",
    "**Trade-off:**\n",
    "- Lowering α (stricter threshold) increases β\n",
    "- Standard: α = 0.05 (5% false positive rate)\n",
    "\n",
    "**In Our Context:**\n",
    "- Type I: Concluding segments differ when they don't\n",
    "- Type II: Missing real segment differences\n",
    "- p < 0.001 gives very low Type I error probability\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.15 Statistical Power\n",
    "\n",
    "**Definition:**\n",
    "Probability of detecting an effect when it truly exists.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{Power} = 1 - \\beta$$\n",
    "\n",
    "**Factors Affecting Power:**\n",
    "1. **Sample size (n):** Larger n → Higher power\n",
    "2. **Effect size:** Larger difference → Higher power\n",
    "3. **Significance level (α):** Higher α → Higher power\n",
    "4. **Variance:** Lower σ → Higher power\n",
    "\n",
    "**In Our Project:**\n",
    "- Large n (4,372 customers)\n",
    "- Large effect size (10x spending difference)\n",
    "- Result: Very high power (almost certain to detect differences)\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.16 Central Limit Theorem (CLT)\n",
    "\n",
    "**Theory:**\n",
    "Sample means approach normal distribution as sample size increases, regardless of population distribution.\n",
    "\n",
    "**Mathematical Statement:**\n",
    "$$\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ as } n \\to \\infty$$\n",
    "\n",
    "**Practical Implication:**\n",
    "- Can use t-tests and ANOVA even with skewed data\n",
    "- Rule of thumb: n > 30 per group\n",
    "- Our project: n > 600 per segment → CLT applies\n",
    "\n",
    "**Why This Matters:**\n",
    "Justifies using parametric tests (ANOVA, t-test) despite non-normal Monetary distribution.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.17 Degrees of Freedom\n",
    "\n",
    "**Definition:**\n",
    "Number of independent values that can vary in calculation.\n",
    "\n",
    "**For ANOVA:**\n",
    "- $df_{between} = k - 1$ (k = number of groups)\n",
    "- $df_{within} = N - k$ (N = total observations)\n",
    "- $df_{total} = N - 1$\n",
    "\n",
    "**Example in Our Project:**\n",
    "- 5 segments (k = 5)\n",
    "- 4,372 customers (N = 4,372)\n",
    "- $df_{between} = 4$\n",
    "- $df_{within} = 4,367$\n",
    "\n",
    "**Why It Matters:**\n",
    "Degrees of freedom determine critical values for hypothesis tests.\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Concepts\n",
    "\n",
    "#### 8.18 Sensitivity Analysis\n",
    "\n",
    "**Theory:**\n",
    "Tests how results change when assumptions or parameters vary.\n",
    "\n",
    "**Purpose:**\n",
    "- Assess robustness\n",
    "- Identify fragile conclusions\n",
    "- Build confidence in results\n",
    "\n",
    "**In Our Project:**\n",
    "- Tested quartiles vs quintiles\n",
    "- Result: 80%+ stability\n",
    "- Conclusion: Segmentation not overly sensitive to binning choice\n",
    "\n",
    "**Types:**\n",
    "1. **One-at-a-time:** Vary one parameter, hold others fixed\n",
    "2. **Global:** Vary all parameters simultaneously (Monte Carlo)\n",
    "3. **Scenario:** Test specific what-if scenarios\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.19 Cross-Validation for Clustering\n",
    "\n",
    "**Challenge:**\n",
    "No ground truth labels for unsupervised learning.\n",
    "\n",
    "**Validation Strategies:**\n",
    "\n",
    "1. **Internal Metrics:**\n",
    "   - Silhouette score\n",
    "   - Davies-Bouldin index\n",
    "   - Calinski-Harabasz index\n",
    "\n",
    "2. **Stability Analysis:**\n",
    "   - Re-run with different random seeds\n",
    "   - Bootstrap resampling\n",
    "   - Compare segment assignments\n",
    "\n",
    "3. **Domain Validation:**\n",
    "   - Do segments make business sense?\n",
    "   - Can we take action on them?\n",
    "\n",
    "**In Our Project:**\n",
    "Combined internal (silhouette), stability (sensitivity), and domain validation (business segments).\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.20 Curse of Dimensionality\n",
    "\n",
    "**Theory:**\n",
    "As dimensions increase, data becomes sparse and distance-based methods fail.\n",
    "\n",
    "**Mathematical Basis:**\n",
    "In high dimensions:\n",
    "- All points become equidistant\n",
    "- $\\lim_{d \\to \\infty} \\frac{\\max(\\text{dist})}{\\min(\\text{dist})} \\to 1$\n",
    "\n",
    "**Implications for Clustering:**\n",
    "- k-means struggles with d > 10-20\n",
    "- Need exponentially more data as d increases\n",
    "\n",
    "**In Our Project:**\n",
    "- Only 3 dimensions (R, F, M)\n",
    "- Well within safe range\n",
    "- Alternative if high-d: Dimensionality reduction (PCA, t-SNE)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Key Theoretical Takeaways\n",
    "\n",
    "**Statistics:**\n",
    "- ANOVA compares multiple group means\n",
    "- Correlation measures linear relationships\n",
    "- CLT justifies parametric tests with large n\n",
    "\n",
    "**Machine Learning:**\n",
    "- K-means minimizes within-cluster variance\n",
    "- Feature scaling essential for distance-based algorithms\n",
    "- Silhouette score measures cluster quality\n",
    "\n",
    "**Data Preprocessing:**\n",
    "- qcut for skewed data (equal frequency)\n",
    "- cut for balanced data (equal width)\n",
    "- Handle missing data based on mechanism\n",
    "\n",
    "**RFM-Specific:**\n",
    "- Invert recency scoring (recent = high score)\n",
    "- Quartiles balance granularity and interpretability\n",
    "- Validate with multiple approaches (ANOVA, k-means, sensitivity)\n",
    "\n",
    "**Interview Strategy:**\n",
    "Don't just say \"I used ANOVA\"—explain **why** (comparing >2 groups), **assumptions** (normality, homogeneity), and **interpretation** (F-statistic, p-value)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
